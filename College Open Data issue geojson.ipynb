{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: geopandas in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.0.1)\n",
      "Requirement already satisfied: shapely in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.0.6)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (4.67.0)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/katellguillou/Library/Python/3.11/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: pyogrio>=0.7.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from geopandas) (0.10.0)\n",
      "Requirement already satisfied: packaging in /Users/katellguillou/Library/Python/3.11/lib/python/site-packages (from geopandas) (24.2)\n",
      "Requirement already satisfied: pyproj>=3.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from geopandas) (3.7.0)\n",
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pyogrio>=0.7.2->geopandas) (2024.8.30)\n",
      "Requirement already satisfied: six>=1.5 in /Users/katellguillou/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    " pip install pandas geopandas shapely tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'null' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 13\u001b[0m\n\u001b[1;32m      1\u001b[0m {\n\u001b[1;32m      2\u001b[0m  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcells\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[1;32m      3\u001b[0m   {\n\u001b[1;32m      4\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcell_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmarkdown\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: {},\n\u001b[1;32m      6\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m# CSV to GeoDataFrame Conversion with Geometry\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis notebook processes the Lyon public transport stops CSV file and adds geometry information.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m    ]\n\u001b[1;32m     10\u001b[0m   },\n\u001b[1;32m     11\u001b[0m   {\n\u001b[1;32m     12\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcell_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m---> 13\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexecution_count\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mnull\u001b[49m,\n\u001b[1;32m     14\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: {},\n\u001b[1;32m     15\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m# Import required libraries\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimport pandas as pd\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimport geopandas as gpd\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom shapely.geometry import Point\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom tqdm.notebook import tqdm\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimport os\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom pathlib import Path\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimport logging\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m# Configure logging\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogging.basicConfig(level=logging.INFO, format=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%(asctime)s\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m%(levelname)s\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m%(message)s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m    ]\n\u001b[1;32m     28\u001b[0m   },\n\u001b[1;32m     29\u001b[0m   {\n\u001b[1;32m     30\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcell_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     31\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexecution_count\u001b[39m\u001b[38;5;124m\"\u001b[39m: null,\n\u001b[1;32m     32\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: {},\n\u001b[1;32m     33\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m# Define file paths\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_file = Path.home() / \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDownloads\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m / \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpoints-arret-lignes-scolaires-reseau-transports-commun-lyonnais.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_file = Path.home() / \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDownloads\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m / \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlyon_transport_stops_with_geometry.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m# Verify input file exists\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif not input_file.exists():\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    raise FileNotFoundError(f\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mInput file not found at \u001b[39m\u001b[38;5;132;01m{input_file}\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     41\u001b[0m    ]\n\u001b[1;32m     42\u001b[0m   },\n\u001b[1;32m     43\u001b[0m   {\n\u001b[1;32m     44\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcell_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     45\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexecution_count\u001b[39m\u001b[38;5;124m\"\u001b[39m: null,\n\u001b[1;32m     46\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: {},\n\u001b[1;32m     47\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdef read_csv_in_chunks(file_path, chunk_size=1000):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mRead CSV file in chunks with error handling\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    try:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        # Get total number of lines for progress bar\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        total_lines = sum(1 for _ in open(file_path, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, encoding=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)) - 1\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        chunks = pd.read_csv(\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m            file_path,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m            chunksize=chunk_size,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m            encoding=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        )\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        return chunks, total_lines\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    except Exception as e:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        logging.error(f\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mError reading CSV file: \u001b[39m\u001b[38;5;132;01m{e}\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        raise\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     63\u001b[0m    ]\n\u001b[1;32m     64\u001b[0m   },\n\u001b[1;32m     65\u001b[0m   {\n\u001b[1;32m     66\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcell_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     67\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexecution_count\u001b[39m\u001b[38;5;124m\"\u001b[39m: null,\n\u001b[1;32m     68\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: {},\n\u001b[1;32m     69\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdef create_geometry(df):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mCreate geometry points from latitude and longitude\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    try:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        geometry = [Point(xy) for xy in zip(df[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlon\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m], df[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlat\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m])]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        return geometry\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    except Exception as e:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        logging.error(f\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mError creating geometry: \u001b[39m\u001b[38;5;132;01m{e}\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        raise\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     78\u001b[0m    ]\n\u001b[1;32m     79\u001b[0m   },\n\u001b[1;32m     80\u001b[0m   {\n\u001b[1;32m     81\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcell_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     82\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexecution_count\u001b[39m\u001b[38;5;124m\"\u001b[39m: null,\n\u001b[1;32m     83\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: {},\n\u001b[1;32m     84\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdef process_data():\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mMain processing function with progress tracking\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    try:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        chunks, total_lines = read_csv_in_chunks(input_file)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        processed_chunks = []\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        # Create progress bar\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        pbar = tqdm(total=total_lines, desc=\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mProcessing rows\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        for chunk in chunks:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m            # Validate lat/lon columns\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m            if \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlat\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not in chunk.columns or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlon\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not in chunk.columns:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m                raise ValueError(\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mRequired columns \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlat\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlon\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not found in CSV\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m            \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m            # Create GeoDataFrame\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m            geometry = create_geometry(chunk)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m            gdf = gpd.GeoDataFrame(chunk, geometry=geometry, crs=\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mEPSG:4326\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m            \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m            # Convert geometry to WKT format for CSV storage\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m            gdf[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeometry\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m] = gdf[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeometry\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m].astype(str)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m            \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m            processed_chunks.append(gdf)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m            pbar.update(len(chunk))\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        pbar.close()\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        # Combine all chunks\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        final_df = pd.concat(processed_chunks, ignore_index=True)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        # Save to CSV\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        final_df.to_csv(output_file, index=False, encoding=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        logging.info(f\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mSuccessfully saved processed data to \u001b[39m\u001b[38;5;132;01m{output_file}\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        return final_df\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    except Exception as e:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        logging.error(f\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mError in data processing: \u001b[39m\u001b[38;5;132;01m{e}\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        raise\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m    ]\n\u001b[1;32m    124\u001b[0m   },\n\u001b[1;32m    125\u001b[0m   {\n\u001b[1;32m    126\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcell_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    127\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexecution_count\u001b[39m\u001b[38;5;124m\"\u001b[39m: null,\n\u001b[1;32m    128\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: {},\n\u001b[1;32m    129\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m# Execute the processing\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtry:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    result_df = process_data()\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    print(f\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mnProcessing complete! File saved at: \u001b[39m\u001b[38;5;132;01m{output_file}\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    print(f\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mnSample of processed data:\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    display(result_df.head())\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexcept Exception as e:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    print(f\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mAn error occurred during processing: \u001b[39m\u001b[38;5;132;01m{e}\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    138\u001b[0m    ]\n\u001b[1;32m    139\u001b[0m   },\n\u001b[1;32m    140\u001b[0m   {\n\u001b[1;32m    141\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcell_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmarkdown\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    142\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: {},\n\u001b[1;32m    143\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m### Validation and Summary Statistics\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m    ]\n\u001b[1;32m    146\u001b[0m   },\n\u001b[1;32m    147\u001b[0m   {\n\u001b[1;32m    148\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcell_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    149\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexecution_count\u001b[39m\u001b[38;5;124m\"\u001b[39m: null,\n\u001b[1;32m    150\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: {},\n\u001b[1;32m    151\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdef display_summary_statistics(df):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mDisplay summary statistics of the processed data\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    print(\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mnSummary Statistics:\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    print(f\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mTotal number of records: \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124mlen(df)}\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    print(f\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mNumber of unique locations: \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124mlen(df.groupby([\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlat\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlon\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]))}\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    print(\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mnColumn information:\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    print(df.info())\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtry:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    display_summary_statistics(result_df)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexcept Exception as e:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    print(f\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mError generating summary statistics: \u001b[39m\u001b[38;5;132;01m{e}\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m    ]\n\u001b[1;32m    165\u001b[0m   }\n\u001b[1;32m    166\u001b[0m  ],\n\u001b[1;32m    167\u001b[0m  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m    168\u001b[0m   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkernelspec\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m    169\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisplay_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPython 3\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    170\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    171\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython3\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    172\u001b[0m   },\n\u001b[1;32m    173\u001b[0m   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlanguage_info\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m    174\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcodemirror_mode\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mipython\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m    177\u001b[0m    },\n\u001b[1;32m    178\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_extension\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.py\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    179\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmimetype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext/x-python\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    180\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    181\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnbconvert_exporter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    182\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpygments_lexer\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mipython3\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    183\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3.8.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    184\u001b[0m   }\n\u001b[1;32m    185\u001b[0m  }\n\u001b[1;32m    186\u001b[0m }\n",
      "\u001b[0;31mNameError\u001b[0m: name 'null' is not defined"
     ]
    }
   ],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# CSV to GeoDataFrame Conversion with Geometry\\n\",\n",
    "    \"This notebook processes the Lyon public transport stops CSV file and adds geometry information.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Import required libraries\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import geopandas as gpd\\n\",\n",
    "    \"from shapely.geometry import Point\\n\",\n",
    "    \"from tqdm.notebook import tqdm\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"from pathlib import Path\\n\",\n",
    "    \"import logging\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Configure logging\\n\",\n",
    "    \"logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Define file paths\\n\",\n",
    "    \"input_file = Path.home() / 'Downloads' / 'points-arret-lignes-scolaires-reseau-transports-commun-lyonnais.csv'\\n\",\n",
    "    \"output_file = Path.home() / 'Downloads' / 'lyon_transport_stops_with_geometry.csv'\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Verify input file exists\\n\",\n",
    "    \"if not input_file.exists():\\n\",\n",
    "    \"    raise FileNotFoundError(f\\\"Input file not found at {input_file}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"def read_csv_in_chunks(file_path, chunk_size=1000):\\n\",\n",
    "    \"    \\\"\\\"\\\"Read CSV file in chunks with error handling\\\"\\\"\\\"\\n\",\n",
    "    \"    try:\\n\",\n",
    "    \"        # Get total number of lines for progress bar\\n\",\n",
    "    \"        total_lines = sum(1 for _ in open(file_path, 'r', encoding='utf-8')) - 1\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        chunks = pd.read_csv(\\n\",\n",
    "    \"            file_path,\\n\",\n",
    "    \"            chunksize=chunk_size,\\n\",\n",
    "    \"            encoding='utf-8'\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        return chunks, total_lines\\n\",\n",
    "    \"    except Exception as e:\\n\",\n",
    "    \"        logging.error(f\\\"Error reading CSV file: {e}\\\")\\n\",\n",
    "    \"        raise\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"def create_geometry(df):\\n\",\n",
    "    \"    \\\"\\\"\\\"Create geometry points from latitude and longitude\\\"\\\"\\\"\\n\",\n",
    "    \"    try:\\n\",\n",
    "    \"        geometry = [Point(xy) for xy in zip(df['lon'], df['lat'])]\\n\",\n",
    "    \"        return geometry\\n\",\n",
    "    \"    except Exception as e:\\n\",\n",
    "    \"        logging.error(f\\\"Error creating geometry: {e}\\\")\\n\",\n",
    "    \"        raise\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"def process_data():\\n\",\n",
    "    \"    \\\"\\\"\\\"Main processing function with progress tracking\\\"\\\"\\\"\\n\",\n",
    "    \"    try:\\n\",\n",
    "    \"        chunks, total_lines = read_csv_in_chunks(input_file)\\n\",\n",
    "    \"        processed_chunks = []\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Create progress bar\\n\",\n",
    "    \"        pbar = tqdm(total=total_lines, desc=\\\"Processing rows\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        for chunk in chunks:\\n\",\n",
    "    \"            # Validate lat/lon columns\\n\",\n",
    "    \"            if 'lat' not in chunk.columns or 'lon' not in chunk.columns:\\n\",\n",
    "    \"                raise ValueError(\\\"Required columns 'lat' and 'lon' not found in CSV\\\")\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Create GeoDataFrame\\n\",\n",
    "    \"            geometry = create_geometry(chunk)\\n\",\n",
    "    \"            gdf = gpd.GeoDataFrame(chunk, geometry=geometry, crs=\\\"EPSG:4326\\\")\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Convert geometry to WKT format for CSV storage\\n\",\n",
    "    \"            gdf['geometry'] = gdf['geometry'].astype(str)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            processed_chunks.append(gdf)\\n\",\n",
    "    \"            pbar.update(len(chunk))\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        pbar.close()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Combine all chunks\\n\",\n",
    "    \"        final_df = pd.concat(processed_chunks, ignore_index=True)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Save to CSV\\n\",\n",
    "    \"        final_df.to_csv(output_file, index=False, encoding='utf-8')\\n\",\n",
    "    \"        logging.info(f\\\"Successfully saved processed data to {output_file}\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        return final_df\\n\",\n",
    "    \"        \\n\",\n",
    "    \"    except Exception as e:\\n\",\n",
    "    \"        logging.error(f\\\"Error in data processing: {e}\\\")\\n\",\n",
    "    \"        raise\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Execute the processing\\n\",\n",
    "    \"try:\\n\",\n",
    "    \"    result_df = process_data()\\n\",\n",
    "    \"    print(f\\\"\\\\nProcessing complete! File saved at: {output_file}\\\")\\n\",\n",
    "    \"    print(f\\\"\\\\nSample of processed data:\\\")\\n\",\n",
    "    \"    display(result_df.head())\\n\",\n",
    "    \"except Exception as e:\\n\",\n",
    "    \"    print(f\\\"An error occurred during processing: {e}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Validation and Summary Statistics\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"def display_summary_statistics(df):\\n\",\n",
    "    \"    \\\"\\\"\\\"Display summary statistics of the processed data\\\"\\\"\\\"\\n\",\n",
    "    \"    print(\\\"\\\\nSummary Statistics:\\\")\\n\",\n",
    "    \"    print(f\\\"Total number of records: {len(df)}\\\")\\n\",\n",
    "    \"    print(f\\\"Number of unique locations: {len(df.groupby(['lat', 'lon']))}\\\")\\n\",\n",
    "    \"    print(\\\"\\\\nColumn information:\\\")\\n\",\n",
    "    \"    print(df.info())\\n\",\n",
    "    \"\\n\",\n",
    "    \"try:\\n\",\n",
    "    \"    display_summary_statistics(result_df)\\n\",\n",
    "    \"except Exception as e:\\n\",\n",
    "    \"    print(f\\\"Error generating summary statistics: {e}\\\")\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.8.0\"\n",
    "  }\n",
    " }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: geopandas in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.0.1)\n",
      "Requirement already satisfied: shapely in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.0.6)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (4.67.0)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/katellguillou/Library/Python/3.11/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: pyogrio>=0.7.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from geopandas) (0.10.0)\n",
      "Requirement already satisfied: packaging in /Users/katellguillou/Library/Python/3.11/lib/python/site-packages (from geopandas) (24.2)\n",
      "Requirement already satisfied: pyproj>=3.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from geopandas) (3.7.0)\n",
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pyogrio>=0.7.2->geopandas) (2024.8.30)\n",
      "Requirement already satisfied: six>=1.5 in /Users/katellguillou/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas geopandas shapely tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "input_file = Path.home() / 'Downloads' / 'points-arret-lignes-scolaires-reseau-transports-commun-lyonnais.csv'\n",
    "output_file = Path.home() / 'Downloads' / 'lyon_transport_stops_with_geometry.csv'\n",
    "\n",
    "# Verify input file exists\n",
    "if not input_file.exists():\n",
    "    raise FileNotFoundError(f\"Input file not found at {input_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_in_chunks(file_path, chunk_size=1000):\n",
    "    \"\"\"Read CSV file in chunks with error handling\"\"\"\n",
    "    try:\n",
    "        # Get total number of lines for progress bar\n",
    "        total_lines = sum(1 for _ in open(file_path, 'r', encoding='utf-8')) - 1\n",
    "        \n",
    "        chunks = pd.read_csv(\n",
    "            file_path,\n",
    "            chunksize=chunk_size,\n",
    "            encoding='utf-8'\n",
    "        )\n",
    "        return chunks, total_lines\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading CSV file: {e}\")\n",
    "        raise\n",
    "\n",
    "def create_geometry(df):\n",
    "    \"\"\"Create geometry points from latitude and longitude\"\"\"\n",
    "    try:\n",
    "        geometry = [Point(xy) for xy in zip(df['lon'], df['lat'])]\n",
    "        return geometry\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error creating geometry: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data():\n",
    "    \"\"\"Main processing function with progress tracking\"\"\"\n",
    "    try:\n",
    "        chunks, total_lines = read_csv_in_chunks(input_file)\n",
    "        processed_chunks = []\n",
    "        \n",
    "        # Create progress bar\n",
    "        pbar = tqdm(total=total_lines, desc=\"Processing rows\")\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            # Validate lat/lon columns\n",
    "            if 'lat' not in chunk.columns or 'lon' not in chunk.columns:\n",
    "                raise ValueError(\"Required columns 'lat' and 'lon' not found in CSV\")\n",
    "            \n",
    "            # Create GeoDataFrame\n",
    "            geometry = create_geometry(chunk)\n",
    "            gdf = gpd.GeoDataFrame(chunk, geometry=geometry, crs=\"EPSG:4326\")\n",
    "            \n",
    "            # Convert geometry to WKT format for CSV storage\n",
    "            gdf['geometry'] = gdf['geometry'].astype(str)\n",
    "            \n",
    "            processed_chunks.append(gdf)\n",
    "            pbar.update(len(chunk))\n",
    "        \n",
    "        pbar.close()\n",
    "        \n",
    "        # Combine all chunks\n",
    "        final_df = pd.concat(processed_chunks, ignore_index=True)\n",
    "        \n",
    "        # Save to CSV\n",
    "        final_df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "        logging.info(f\"Successfully saved processed data to {output_file}\")\n",
    "        \n",
    "        return final_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in data processing: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 11:52:03,411 - ERROR - Error in data processing: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "Exception ignored in: <function tqdm.__del__ at 0x10ef83380>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/std.py\", line 1148, in __del__\n",
      "    self.close()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/notebook.py\", line 279, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "    ^^^^^^^^^\n",
      "AttributeError: 'tqdm_notebook' object has no attribute 'disp'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred during processing: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n"
     ]
    }
   ],
   "source": [
    "# Execute the processing\n",
    "try:\n",
    "    result_df = process_data()\n",
    "    print(f\"\\nProcessing complete! File saved at: {output_file}\")\n",
    "    print(f\"\\nSample of processed data:\")\n",
    "    display(result_df.head())\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during processing: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from tqdm import tqdm  # Changed from tqdm.notebook\n",
    "import os\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data():\n",
    "    \"\"\"Main processing function with progress tracking\"\"\"\n",
    "    try:\n",
    "        chunks, total_lines = read_csv_in_chunks(input_file)\n",
    "        processed_chunks = []\n",
    "        \n",
    "        # Process chunks with simple progress counter\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            # Validate lat/lon columns\n",
    "            if 'lat' not in chunk.columns or 'lon' not in chunk.columns:\n",
    "                raise ValueError(\"Required columns 'lat' and 'lon' not found in CSV\")\n",
    "            \n",
    "            # Create GeoDataFrame\n",
    "            geometry = create_geometry(chunk)\n",
    "            gdf = gpd.GeoDataFrame(chunk, geometry=geometry, crs=\"EPSG:4326\")\n",
    "            \n",
    "            # Convert geometry to WKT format for CSV storage\n",
    "            gdf['geometry'] = gdf['geometry'].astype(str)\n",
    "            \n",
    "            processed_chunks.append(gdf)\n",
    "            \n",
    "            # Simple progress printing\n",
    "            processed_rows = (i + 1) * 1000\n",
    "            print(f\"\\rProcessing: {min(processed_rows, total_lines)}/{total_lines} rows\", end=\"\")\n",
    "        \n",
    "        print(\"\\nCombining processed chunks...\")\n",
    "        \n",
    "        # Combine all chunks\n",
    "        final_df = pd.concat(processed_chunks, ignore_index=True)\n",
    "        \n",
    "        # Save to CSV\n",
    "        print(\"Saving to CSV...\")\n",
    "        final_df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "        logging.info(f\"Successfully saved processed data to {output_file}\")\n",
    "        \n",
    "        return final_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in data processing: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 11:53:57,012 - ERROR - Error in data processing: Error tokenizing data. C error: Expected 3 fields in line 4, saw 4\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: Error tokenizing data. C error: Expected 3 fields in line 4, saw 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Execute the processing and display summary\n",
    "try:\n",
    "    # Process the data\n",
    "    result_df = process_data()\n",
    "    print(f\"\\nProcessing complete! File saved at: {output_file}\")\n",
    "    print(f\"\\nSample of processed data:\")\n",
    "    display(result_df.head())\n",
    "    \n",
    "    # Display summary statistics\n",
    "    print(\"\\nSummary Statistics:\")\n",
    "    print(f\"Total number of records: {len(result_df)}\")\n",
    "    print(f\"Number of unique locations: {len(result_df.groupby(['lat', 'lon']))}\")\n",
    "    print(\"\\nColumn information:\")\n",
    "    print(result_df.info())\n",
    "    \n",
    "    # Additional useful statistics\n",
    "    print(\"\\nMemory usage:\")\n",
    "    print(result_df.memory_usage(deep=True).sum() / 1024**2, \"MB\")\n",
    "    print(\"\\nMissing values summary:\")\n",
    "    print(result_df.isnull().sum())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting CSV file structure...\n",
      "First 5 lines of the file:\n",
      "Line 1: id;nom;desserte;pmr;ascenseur;escalator;gid;last_update;last_update_fme;adresse;localise_face_a_adresse;commune;insee;lon;lat\n",
      "Number of fields: 1\n",
      "Line 2: 46945;Fortunat - Indiennerie;J58:R;False;False;False;1953;2024-12-04 02:02:10+01:00;2024-12-04 03:20:12.248077+01:00;ROUTE DE SAINT FORTUNAT;False;Saint-Cyr-au-Mont-d'Or;69191;4,809544825633801;45,80757185491305\n",
      "Number of fields: 3\n",
      "Line 3: 73;Aqueducs de Beaunant;J12:A;False;False;False;2260;2024-12-04 02:02:10+01:00;2024-12-04 03:20:12.117408+01:00;133BIS AVENUE DE L AQUEDUC DE BEAUNANT;False;Sainte-Foy-ls-Lyon;69202;4,779963362674723;45,72432459823616\n",
      "Number of fields: 3\n",
      "Line 4: 48128;Le Four;J76:A,J76:R;False;False;False;2240;2024-12-04 02:02:10+01:00;2024-12-04 03:20:12.262558+01:00;1156 CHEMIN DU FOUR;False;Cailloux-sur-Fontaines;69033;4,876594070135879;45,856570219481455\n",
      "Number of fields: 4\n",
      "Line 5: 48271;Fontrobert;J133:A,J133:R,J3:A;False;False;False;2247;2024-12-04 02:02:10+01:00;2024-12-04 03:20:12.263129+01:00;35 RUE DU 23 AOUT 1944;False;Mions;69283;4,94924410028283;45,675036422408105\n",
      "Number of fields: 5\n",
      "\n",
      "Possible semicolon delimiter detected\n",
      "\n",
      "Detected columns: ['id;nom;desserte;pmr;ascenseur;escalator;gid;last_update;last_update_fme;adresse;localise_face_a_adresse;commune;insee;lon;lat']\n"
     ]
    }
   ],
   "source": [
    "def inspect_csv_file(file_path):\n",
    "    \"\"\"Inspect the CSV file structure\"\"\"\n",
    "    try:\n",
    "        # Read first few lines to detect the format\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            print(\"First 5 lines of the file:\")\n",
    "            for i, line in enumerate(f):\n",
    "                if i < 5:\n",
    "                    print(f\"Line {i+1}: {line.strip()}\")\n",
    "                    print(f\"Number of fields: {len(line.strip().split(','))}\")\n",
    "                else:\n",
    "                    break\n",
    "                    \n",
    "        # Try to detect delimiter\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            first_line = f.readline()\n",
    "            if ';' in first_line:\n",
    "                print(\"\\nPossible semicolon delimiter detected\")\n",
    "            elif ',' in first_line:\n",
    "                print(\"\\nPossible comma delimiter detected\")\n",
    "                \n",
    "        return pd.read_csv(file_path, nrows=1, encoding='utf-8').columns.tolist()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error inspecting file: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"Inspecting CSV file structure...\")\n",
    "columns = inspect_csv_file(input_file)\n",
    "if columns:\n",
    "    print(\"\\nDetected columns:\", columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_in_chunks(file_path, chunk_size=1000):\n",
    "    \"\"\"Read CSV file in chunks with error handling\"\"\"\n",
    "    try:\n",
    "        # Read with explicit semicolon delimiter\n",
    "        chunks = pd.read_csv(\n",
    "            file_path,\n",
    "            chunksize=chunk_size,\n",
    "            encoding='utf-8',\n",
    "            delimiter=';',  # Explicit semicolon delimiter\n",
    "            decimal=',',    # Handle European decimal format if present\n",
    "            on_bad_lines='warn'\n",
    "        )\n",
    "        \n",
    "        # Get total number of lines\n",
    "        total_lines = sum(1 for _ in open(file_path, 'r', encoding='utf-8')) - 1\n",
    "        \n",
    "        return chunks, total_lines\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading CSV file: {e}\")\n",
    "        raise\n",
    "\n",
    "def create_geometry(df):\n",
    "    \"\"\"Create geometry points from latitude and longitude\"\"\"\n",
    "    try:\n",
    "        # Convert coordinates to float, handling potential string formatting\n",
    "        df['lon'] = pd.to_numeric(df['lon'].str.replace(',', '.'), errors='coerce')\n",
    "        df['lat'] = pd.to_numeric(df['lat'].str.replace(',', '.'), errors='coerce')\n",
    "        \n",
    "        # Create geometry points\n",
    "        geometry = [Point(xy) for xy in zip(df['lon'], df['lat'])]\n",
    "        return geometry\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error creating geometry: {e}\")\n",
    "        raise\n",
    "\n",
    "def validate_dataframe(df):\n",
    "    \"\"\"Validate dataframe structure and content\"\"\"\n",
    "    required_columns = ['id', 'nom', 'lat', 'lon']\n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    \n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "    \n",
    "    # Check for null values in critical columns\n",
    "    null_counts = df[['lat', 'lon']].isnull().sum()\n",
    "    if null_counts.any():\n",
    "        logging.warning(f\"Found null values in coordinates:\\n{null_counts}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data():\n",
    "    \"\"\"Main processing function with progress tracking\"\"\"\n",
    "    try:\n",
    "        chunks, total_lines = read_csv_in_chunks(input_file)\n",
    "        processed_chunks = []\n",
    "        \n",
    "        print(f\"Processing {total_lines} rows...\")\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            # Validate chunk data\n",
    "            validate_dataframe(chunk)\n",
    "            \n",
    "            # Create GeoDataFrame\n",
    "            geometry = create_geometry(chunk)\n",
    "            gdf = gpd.GeoDataFrame(chunk, geometry=geometry, crs=\"EPSG:4326\")\n",
    "            \n",
    "            # Convert geometry to WKT format for CSV storage\n",
    "            gdf['geometry'] = gdf['geometry'].astype(str)\n",
    "            \n",
    "            processed_chunks.append(gdf)\n",
    "            \n",
    "            # Progress\n",
    "            processed_rows = (i + 1) * 1000\n",
    "            print(f\"\\rProcessed: {min(processed_rows, total_lines)}/{total_lines} rows\", end=\"\")\n",
    "        \n",
    "        print(\"\\nCombining processed chunks...\")\n",
    "        \n",
    "        # Combine all chunks\n",
    "        final_df = pd.concat(processed_chunks, ignore_index=True)\n",
    "        \n",
    "        # Save to CSV\n",
    "        print(\"Saving to CSV...\")\n",
    "        final_df.to_csv(output_file, index=False, encoding='utf-8', sep=';')\n",
    "        logging.info(f\"Successfully saved processed data to {output_file}\")\n",
    "        \n",
    "        return final_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in data processing: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 11:55:39,834 - ERROR - Error creating geometry: Can only use .str accessor with string values!\n",
      "2024-12-04 11:55:39,835 - ERROR - Error in data processing: Can only use .str accessor with string values!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2143 rows...\n",
      "An error occurred: Can only use .str accessor with string values!\n"
     ]
    }
   ],
   "source": [
    "# Execute the processing and display summary\n",
    "try:\n",
    "    # Process the data\n",
    "    result_df = process_data()\n",
    "    print(f\"\\nProcessing complete! File saved at: {output_file}\")\n",
    "    \n",
    "    # Display sample and statistics\n",
    "    print(\"\\nSample of processed data:\")\n",
    "    display(result_df.head())\n",
    "    \n",
    "    print(\"\\nSummary Statistics:\")\n",
    "    print(f\"Total number of records: {len(result_df)}\")\n",
    "    print(f\"Number of unique locations: {len(result_df.groupby(['lat', 'lon']))}\")\n",
    "    \n",
    "    print(\"\\nColumn information:\")\n",
    "    print(result_df.info())\n",
    "    \n",
    "    # Display coordinate ranges\n",
    "    print(\"\\nCoordinate Ranges:\")\n",
    "    print(\"Latitude range:\", result_df['lat'].min(), \"to\", result_df['lat'].max())\n",
    "    print(\"Longitude range:\", result_df['lon'].min(), \"to\", result_df['lon'].max())\n",
    "    \n",
    "    # Check for any remaining issues\n",
    "    print(\"\\nMissing values summary:\")\n",
    "    print(result_df.isnull().sum())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "input_file = Path.home() / 'Downloads' / 'points-arret-lignes-scolaires-reseau-transports-commun-lyonnais.csv'\n",
    "output_file = Path.home() / 'Downloads' / 'lyon_transport_stops_with_geometry.csv'\n",
    "\n",
    "# Verify input file exists\n",
    "if not input_file.exists():\n",
    "    raise FileNotFoundError(f\"Input file not found at {input_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_in_chunks(file_path, chunk_size=1000):\n",
    "    \"\"\"Read CSV file in chunks with error handling\"\"\"\n",
    "    try:\n",
    "        # Read with explicit semicolon delimiter\n",
    "        chunks = pd.read_csv(\n",
    "            file_path,\n",
    "            chunksize=chunk_size,\n",
    "            encoding='utf-8',\n",
    "            delimiter=';',  # Explicit semicolon delimiter\n",
    "            decimal=',',    # Handle European decimal format if present\n",
    "            on_bad_lines='warn'\n",
    "        )\n",
    "        \n",
    "        # Get total number of lines\n",
    "        total_lines = sum(1 for _ in open(file_path, 'r', encoding='utf-8')) - 1\n",
    "        \n",
    "        return chunks, total_lines\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading CSV file: {e}\")\n",
    "        raise\n",
    "\n",
    "def create_geometry(df):\n",
    "    \"\"\"Create geometry points from latitude and longitude\"\"\"\n",
    "    try:\n",
    "        # Convert coordinates to float, handling potential string formatting\n",
    "        df['lon'] = pd.to_numeric(df['lon'].str.replace(',', '.'), errors='coerce')\n",
    "        df['lat'] = pd.to_numeric(df['lat'].str.replace(',', '.'), errors='coerce')\n",
    "        \n",
    "        # Create geometry points\n",
    "        geometry = [Point(xy) for xy in zip(df['lon'], df['lat'])]\n",
    "        return geometry\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error creating geometry: {e}\")\n",
    "        raise\n",
    "\n",
    "def validate_dataframe(df):\n",
    "    \"\"\"Validate dataframe structure and content\"\"\"\n",
    "    required_columns = ['id', 'nom', 'lat', 'lon']\n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    \n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "    \n",
    "    # Check for null values in critical columns\n",
    "    null_counts = df[['lat', 'lon']].isnull().sum()\n",
    "    if null_counts.any():\n",
    "        logging.warning(f\"Found null values in coordinates:\\n{null_counts}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data():\n",
    "    \"\"\"Main processing function with progress tracking\"\"\"\n",
    "    try:\n",
    "        chunks, total_lines = read_csv_in_chunks(input_file)\n",
    "        processed_chunks = []\n",
    "        \n",
    "        print(f\"Processing {total_lines} rows...\")\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            # Validate chunk data\n",
    "            validate_dataframe(chunk)\n",
    "            \n",
    "            # Create GeoDataFrame\n",
    "            geometry = create_geometry(chunk)\n",
    "            gdf = gpd.GeoDataFrame(chunk, geometry=geometry, crs=\"EPSG:4326\")\n",
    "            \n",
    "            # Convert geometry to WKT format for CSV storage\n",
    "            gdf['geometry'] = gdf['geometry'].astype(str)\n",
    "            \n",
    "            processed_chunks.append(gdf)\n",
    "            \n",
    "            # Progress\n",
    "            processed_rows = (i + 1) * 1000\n",
    "            print(f\"\\rProcessed: {min(processed_rows, total_lines)}/{total_lines} rows\", end=\"\")\n",
    "        \n",
    "        print(\"\\nCombining processed chunks...\")\n",
    "        \n",
    "        # Combine all chunks\n",
    "        final_df = pd.concat(processed_chunks, ignore_index=True)\n",
    "        \n",
    "        # Save to CSV\n",
    "        print(\"Saving to CSV...\")\n",
    "        final_df.to_csv(output_file, index=False, encoding='utf-8', sep=';')\n",
    "        logging.info(f\"Successfully saved processed data to {output_file}\")\n",
    "        \n",
    "        return final_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in data processing: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 11:57:04,004 - ERROR - Error creating geometry: Can only use .str accessor with string values!\n",
      "2024-12-04 11:57:04,004 - ERROR - Error in data processing: Can only use .str accessor with string values!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2143 rows...\n",
      "An error occurred: Can only use .str accessor with string values!\n"
     ]
    }
   ],
   "source": [
    "# Execute the processing and display summary\n",
    "try:\n",
    "    # Process the data\n",
    "    result_df = process_data()\n",
    "    print(f\"\\nProcessing complete! File saved at: {output_file}\")\n",
    "    \n",
    "    # Display sample and statistics\n",
    "    print(\"\\nSample of processed data:\")\n",
    "    display(result_df.head())\n",
    "    \n",
    "    print(\"\\nSummary Statistics:\")\n",
    "    print(f\"Total number of records: {len(result_df)}\")\n",
    "    print(f\"Number of unique locations: {len(result_df.groupby(['lat', 'lon']))}\")\n",
    "    \n",
    "    print(\"\\nColumn information:\")\n",
    "    print(result_df.info())\n",
    "    \n",
    "    # Display coordinate ranges\n",
    "    print(\"\\nCoordinate Ranges:\")\n",
    "    print(\"Latitude range:\", result_df['lat'].min(), \"to\", result_df['lat'].max())\n",
    "    print(\"Longitude range:\", result_df['lon'].min(), \"to\", result_df['lon'].max())\n",
    "    \n",
    "    # Check for any remaining issues\n",
    "    print(\"\\nMissing values summary:\")\n",
    "    print(result_df.isnull().sum())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 11:57:41,770 - ERROR - Error in data processing: could not convert string to float: np.str_('4,809544825633801')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processing...\n",
      "Reading CSV file...\n",
      "Processing 2143 rows...\n",
      "An error occurred: could not convert string to float: np.str_('4,809544825633801')\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Define file paths\n",
    "input_file = Path.home() / 'Downloads' / 'points-arret-lignes-scolaires-reseau-transports-commun-lyonnais.csv'\n",
    "output_file = Path.home() / 'Downloads' / 'lyon_transport_stops_with_geometry.csv'\n",
    "\n",
    "# Main processing function\n",
    "def process_data():\n",
    "    try:\n",
    "        # Read the CSV file\n",
    "        print(\"Reading CSV file...\")\n",
    "        df = pd.read_csv(input_file, delimiter=';', encoding='utf-8')\n",
    "        \n",
    "        print(f\"Processing {len(df)} rows...\")\n",
    "        \n",
    "        # Create geometry points\n",
    "        geometry = [Point(x, y) for x, y in zip(df['lon'], df['lat'])]\n",
    "        \n",
    "        # Create GeoDataFrame\n",
    "        gdf = gpd.GeoDataFrame(df, geometry=geometry, crs=\"EPSG:4326\")\n",
    "        \n",
    "        # Convert geometry to WKT format for CSV storage\n",
    "        gdf['geometry'] = gdf['geometry'].astype(str)\n",
    "        \n",
    "        # Save to CSV\n",
    "        print(\"Saving to CSV...\")\n",
    "        gdf.to_csv(output_file, index=False, encoding='utf-8', sep=';')\n",
    "        \n",
    "        return gdf\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in data processing: {e}\")\n",
    "        raise\n",
    "\n",
    "# Execute and display results\n",
    "try:\n",
    "    # Process the data\n",
    "    print(\"Starting processing...\")\n",
    "    result_df = process_data()\n",
    "    print(f\"\\nProcessing complete! File saved at: {output_file}\")\n",
    "    \n",
    "    # Display sample\n",
    "    print(\"\\nFirst 5 rows of processed data:\")\n",
    "    display(result_df.head())\n",
    "    \n",
    "    # Display statistics\n",
    "    print(\"\\nDataset Statistics:\")\n",
    "    print(f\"Total records: {len(result_df)}\")\n",
    "    print(f\"Unique locations: {len(result_df.groupby(['lat', 'lon']))}\")\n",
    "    \n",
    "    print(\"\\nColumns in dataset:\")\n",
    "    for col in result_df.columns:\n",
    "        print(f\"- {col}\")\n",
    "    \n",
    "    print(\"\\nCoordinate Ranges:\")\n",
    "    print(f\"Latitude:  {result_df['lat'].min():.6f} to {result_df['lat'].max():.6f}\")\n",
    "    print(f\"Longitude: {result_df['lon'].min():.6f} to {result_df['lon'].max():.6f}\")\n",
    "    \n",
    "    print(\"\\nMemory Usage:\")\n",
    "    print(f\"{result_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # Validate data quality\n",
    "    print(\"\\nMissing Values:\")\n",
    "    missing = result_df.isnull().sum()\n",
    "    if missing.any():\n",
    "        print(missing[missing > 0])\n",
    "    else:\n",
    "        print(\"No missing values found\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 11:58:16,653 - WARNING - Found 2143 rows with invalid coordinates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processing...\n",
      "Reading CSV file...\n",
      "Found 2143 rows\n",
      "Validating coordinates...\n",
      "Retained 0 valid rows\n",
      "Creating geometry...\n",
      "Saving to CSV...\n",
      "\n",
      "Processing complete! File saved at: /Users/katellguillou/Downloads/lyon_transport_stops_with_geometry.csv\n",
      "\n",
      "Dataset Analysis:\n",
      "--------------------------------------------------\n",
      "\n",
      "Basic Statistics:\n",
      "Total records: 0\n",
      "Unique locations: 0\n",
      "\n",
      "Coordinate Ranges:\n",
      "Latitude:  nan to nan\n",
      "Longitude: nan to nan\n",
      "\n",
      "Columns in dataset:\n",
      "An error occurred: unsupported format string passed to numpy.dtypes.Int64DType.__format__\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to numpy.dtypes.Int64DType.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 104\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mProcessing complete! File saved at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# Analyze results\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m \u001b[43manalyze_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Display sample\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSample of processed data:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[32], line 89\u001b[0m, in \u001b[0;36manalyze_results\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     87\u001b[0m     non_null \u001b[38;5;241m=\u001b[39m df[col]\u001b[38;5;241m.\u001b[39mcount()\n\u001b[1;32m     88\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m df[col]\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m---> 89\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m- \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m<20\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m<10\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Non-null: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnon_null\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>5\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMemory Usage:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     92\u001b[0m memory_usage \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mmemory_usage(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1024\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported format string passed to numpy.dtypes.Int64DType.__format__"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Define file paths\n",
    "input_file = Path.home() / 'Downloads' / 'points-arret-lignes-scolaires-reseau-transports-commun-lyonnais.csv'\n",
    "output_file = Path.home() / 'Downloads' / 'lyon_transport_stops_with_geometry.csv'\n",
    "\n",
    "def validate_coordinates(df):\n",
    "    \"\"\"Validate coordinate data\"\"\"\n",
    "    # Check if coordinates are present\n",
    "    if 'lat' not in df.columns or 'lon' not in df.columns:\n",
    "        raise ValueError(\"Missing coordinate columns (lat/lon)\")\n",
    "    \n",
    "    # Convert to numeric if needed\n",
    "    df['lat'] = pd.to_numeric(df['lat'], errors='coerce')\n",
    "    df['lon'] = pd.to_numeric(df['lon'], errors='coerce')\n",
    "    \n",
    "    # Check for valid coordinate ranges\n",
    "    valid_mask = (\n",
    "        (df['lat'] >= -90) & (df['lat'] <= 90) &\n",
    "        (df['lon'] >= -180) & (df['lon'] <= 180)\n",
    "    )\n",
    "    \n",
    "    invalid_coords = (~valid_mask).sum()\n",
    "    if invalid_coords > 0:\n",
    "        logging.warning(f\"Found {invalid_coords} rows with invalid coordinates\")\n",
    "    \n",
    "    return df[valid_mask]\n",
    "\n",
    "def process_data():\n",
    "    try:\n",
    "        # Read the CSV file\n",
    "        print(\"Reading CSV file...\")\n",
    "        df = pd.read_csv(input_file, delimiter=';', encoding='utf-8')\n",
    "        print(f\"Found {len(df)} rows\")\n",
    "        \n",
    "        # Validate and clean coordinates\n",
    "        print(\"Validating coordinates...\")\n",
    "        df = validate_coordinates(df)\n",
    "        print(f\"Retained {len(df)} valid rows\")\n",
    "        \n",
    "        # Create geometry points\n",
    "        print(\"Creating geometry...\")\n",
    "        geometry = [Point(x, y) for x, y in zip(df['lon'], df['lat'])]\n",
    "        \n",
    "        # Create GeoDataFrame\n",
    "        gdf = gpd.GeoDataFrame(\n",
    "            df,\n",
    "            geometry=geometry,\n",
    "            crs=\"EPSG:4326\"\n",
    "        )\n",
    "        \n",
    "        # Convert geometry to WKT format for CSV storage\n",
    "        gdf['geometry'] = gdf['geometry'].astype(str)\n",
    "        \n",
    "        # Save to CSV\n",
    "        print(\"Saving to CSV...\")\n",
    "        gdf.to_csv(output_file, index=False, encoding='utf-8', sep=';')\n",
    "        \n",
    "        return gdf\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in data processing: {e}\")\n",
    "        raise\n",
    "\n",
    "def analyze_results(df):\n",
    "    \"\"\"Analyze and display results\"\"\"\n",
    "    print(\"\\nDataset Analysis:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    print(\"\\nBasic Statistics:\")\n",
    "    print(f\"Total records: {len(df)}\")\n",
    "    print(f\"Unique locations: {len(df.groupby(['lat', 'lon']))}\")\n",
    "    \n",
    "    print(\"\\nCoordinate Ranges:\")\n",
    "    print(f\"Latitude:  {df['lat'].min():.6f} to {df['lat'].max():.6f}\")\n",
    "    print(f\"Longitude: {df['lon'].min():.6f} to {df['lon'].max():.6f}\")\n",
    "    \n",
    "    print(\"\\nColumns in dataset:\")\n",
    "    for col in df.columns:\n",
    "        non_null = df[col].count()\n",
    "        dtype = df[col].dtype\n",
    "        print(f\"- {col:<20} | Type: {dtype:<10} | Non-null: {non_null:>5}\")\n",
    "    \n",
    "    print(\"\\nMemory Usage:\")\n",
    "    memory_usage = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    print(f\"{memory_usage:.2f} MB\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Execute everything\n",
    "try:\n",
    "    print(\"Starting processing...\")\n",
    "    result_df = process_data()\n",
    "    print(f\"\\nProcessing complete! File saved at: {output_file}\")\n",
    "    \n",
    "    # Analyze results\n",
    "    analyze_results(result_df)\n",
    "    \n",
    "    # Display sample\n",
    "    print(\"\\nSample of processed data:\")\n",
    "    display(result_df.head())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 11:59:08,272 - WARNING - Found 2143 rows outside Lyon area coordinates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processing...\n",
      "Reading CSV file...\n",
      "Found 2143 rows\n",
      "Validating coordinates...\n",
      "Processing 2143 rows\n",
      "Creating geometry...\n",
      "Saving to CSV...\n",
      "\n",
      "Processing complete! File saved at: /Users/katellguillou/Downloads/lyon_transport_stops_with_geometry.csv\n",
      "\n",
      "Dataset Analysis:\n",
      "--------------------------------------------------\n",
      "\n",
      "Basic Statistics:\n",
      "Total records: 2143\n",
      "Unique locations: 1\n",
      "\n",
      "Coordinate Ranges:\n",
      "Latitude:  nan to nan\n",
      "Longitude: nan to nan\n",
      "\n",
      "Columns in dataset:\n",
      "- id                   | Type: int64           | Non-null: 2143\n",
      "- nom                  | Type: object          | Non-null: 2143\n",
      "- desserte             | Type: object          | Non-null: 2143\n",
      "- pmr                  | Type: bool            | Non-null: 2143\n",
      "- ascenseur            | Type: bool            | Non-null: 2143\n",
      "- escalator            | Type: bool            | Non-null: 2143\n",
      "- gid                  | Type: int64           | Non-null: 2143\n",
      "- last_update          | Type: object          | Non-null: 2143\n",
      "- last_update_fme      | Type: object          | Non-null: 2143\n",
      "- adresse              | Type: object          | Non-null: 2143\n",
      "- localise_face_a_adresse | Type: bool            | Non-null: 2143\n",
      "- commune              | Type: object          | Non-null: 2143\n",
      "- insee                | Type: int64           | Non-null: 2143\n",
      "- lon                  | Type: float64         | Non-null: 0\n",
      "- lat                  | Type: float64         | Non-null: 0\n",
      "- geometry             | Type: object          | Non-null: 2143\n",
      "\n",
      "Memory Usage:\n",
      "1.19 MB\n",
      "\n",
      "Coordinate Distribution:\n",
      "Latitude quartiles:\n",
      "count    0.0\n",
      "mean     NaN\n",
      "std      NaN\n",
      "min      NaN\n",
      "25%      NaN\n",
      "50%      NaN\n",
      "75%      NaN\n",
      "max      NaN\n",
      "Name: lat, dtype: float64\n",
      "\n",
      "Longitude quartiles:\n",
      "count    0.0\n",
      "mean     NaN\n",
      "std      NaN\n",
      "min      NaN\n",
      "25%      NaN\n",
      "50%      NaN\n",
      "75%      NaN\n",
      "max      NaN\n",
      "Name: lon, dtype: float64\n",
      "\n",
      "Sample of processed data:\n",
      "      id                     nom            desserte    pmr  ascenseur  escalator   gid                last_update                   last_update_fme                                 adresse  localise_face_a_adresse                 commune  insee  lon  lat     geometry\n",
      "0  46945  Fortunat - Indiennerie               J58:R  False      False      False  1953  2024-12-04 02:02:10+01:00  2024-12-04 03:20:12.248077+01:00                 ROUTE DE SAINT FORTUNAT                    False  Saint-Cyr-au-Mont-d'Or  69191  NaN  NaN  POINT EMPTY\n",
      "1     73    Aqueducs de Beaunant               J12:A  False      False      False  2260  2024-12-04 02:02:10+01:00  2024-12-04 03:20:12.117408+01:00  133BIS AVENUE DE L AQUEDUC DE BEAUNANT                    False     Sainte-Foy-ls-Lyon  69202  NaN  NaN  POINT EMPTY\n",
      "2  48128                 Le Four         J76:A,J76:R  False      False      False  2240  2024-12-04 02:02:10+01:00  2024-12-04 03:20:12.262558+01:00                     1156 CHEMIN DU FOUR                    False  Cailloux-sur-Fontaines  69033  NaN  NaN  POINT EMPTY\n",
      "3  48271              Fontrobert  J133:A,J133:R,J3:A  False      False      False  2247  2024-12-04 02:02:10+01:00  2024-12-04 03:20:12.263129+01:00                  35 RUE DU 23 AOUT 1944                    False                   Mions  69283  NaN  NaN  POINT EMPTY\n",
      "4  50079           Peri - Picard              J396:A  False      False      False  2259  2024-12-04 02:02:10+01:00  2024-12-04 03:20:12.263256+01:00                       86 RUE PARMENTIER                    False              Saint-Fons  69199  NaN  NaN  POINT EMPTY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rc/xkkmd5711dl9yqc5qv9_ltyw0000gn/T/ipykernel_62251/1393327157.py:56: UserWarning: Geometry column does not contain geometry.\n",
      "  gdf['geometry'] = gdf['geometry'].astype(str)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Define file paths\n",
    "input_file = Path.home() / 'Downloads' / 'points-arret-lignes-scolaires-reseau-transports-commun-lyonnais.csv'\n",
    "output_file = Path.home() / 'Downloads' / 'lyon_transport_stops_with_geometry.csv'\n",
    "\n",
    "def validate_coordinates(df):\n",
    "    \"\"\"Validate coordinate data\"\"\"\n",
    "    # Convert to numeric if needed\n",
    "    df['lat'] = pd.to_numeric(df['lat'], errors='coerce')\n",
    "    df['lon'] = pd.to_numeric(df['lon'], errors='coerce')\n",
    "    \n",
    "    # Check for valid coordinate ranges for Lyon area\n",
    "    valid_mask = (\n",
    "        (df['lat'] >= 45.5) & (df['lat'] <= 46.0) &  # Lyon latitude range\n",
    "        (df['lon'] >= 4.5) & (df['lon'] <= 5.0)      # Lyon longitude range\n",
    "    )\n",
    "    \n",
    "    invalid_coords = (~valid_mask).sum()\n",
    "    if invalid_coords > 0:\n",
    "        logging.warning(f\"Found {invalid_coords} rows outside Lyon area coordinates\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def process_data():\n",
    "    try:\n",
    "        # Read the CSV file\n",
    "        print(\"Reading CSV file...\")\n",
    "        df = pd.read_csv(input_file, delimiter=';', encoding='utf-8')\n",
    "        print(f\"Found {len(df)} rows\")\n",
    "        \n",
    "        # Validate and clean coordinates\n",
    "        print(\"Validating coordinates...\")\n",
    "        df = validate_coordinates(df)\n",
    "        print(f\"Processing {len(df)} rows\")\n",
    "        \n",
    "        # Create geometry points\n",
    "        print(\"Creating geometry...\")\n",
    "        geometry = [Point(x, y) for x, y in zip(df['lon'], df['lat'])]\n",
    "        \n",
    "        # Create GeoDataFrame\n",
    "        gdf = gpd.GeoDataFrame(\n",
    "            df,\n",
    "            geometry=geometry,\n",
    "            crs=\"EPSG:4326\"\n",
    "        )\n",
    "        \n",
    "        # Convert geometry to WKT format for CSV storage\n",
    "        gdf['geometry'] = gdf['geometry'].astype(str)\n",
    "        \n",
    "        # Save to CSV\n",
    "        print(\"Saving to CSV...\")\n",
    "        gdf.to_csv(output_file, index=False, encoding='utf-8', sep=';')\n",
    "        \n",
    "        return gdf\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in data processing: {e}\")\n",
    "        raise\n",
    "\n",
    "def analyze_results(df):\n",
    "    \"\"\"Analyze and display results\"\"\"\n",
    "    print(\"\\nDataset Analysis:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    print(\"\\nBasic Statistics:\")\n",
    "    print(f\"Total records: {len(df)}\")\n",
    "    print(f\"Unique locations: {len(df.groupby(['lat', 'lon']))}\")\n",
    "    \n",
    "    print(\"\\nCoordinate Ranges:\")\n",
    "    print(f\"Latitude:  {df['lat'].min():.6f} to {df['lat'].max():.6f}\")\n",
    "    print(f\"Longitude: {df['lon'].min():.6f} to {df['lon'].max():.6f}\")\n",
    "    \n",
    "    print(\"\\nColumns in dataset:\")\n",
    "    for col in df.columns:\n",
    "        non_null = df[col].count()\n",
    "        dtype = str(df[col].dtype)  # Convert dtype to string to avoid formatting issues\n",
    "        print(f\"- {col:<20} | Type: {dtype:<15} | Non-null: {non_null}\")\n",
    "    \n",
    "    print(\"\\nMemory Usage:\")\n",
    "    memory_usage = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    print(f\"{memory_usage:.2f} MB\")\n",
    "    \n",
    "    # Add coordinate distribution information\n",
    "    print(\"\\nCoordinate Distribution:\")\n",
    "    print(\"Latitude quartiles:\")\n",
    "    print(df['lat'].describe())\n",
    "    print(\"\\nLongitude quartiles:\")\n",
    "    print(df['lon'].describe())\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Execute everything\n",
    "try:\n",
    "    print(\"Starting processing...\")\n",
    "    result_df = process_data()\n",
    "    print(f\"\\nProcessing complete! File saved at: {output_file}\")\n",
    "    \n",
    "    # Analyze results\n",
    "    analyze_results(result_df)\n",
    "    \n",
    "    # Display sample\n",
    "    print(\"\\nSample of processed data:\")\n",
    "    print(result_df.head().to_string())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processing...\n",
      "Reading and cleaning CSV file...\n",
      "Found 2143 rows\n",
      "Creating geometry...\n",
      "Saving to GeoJSON: /Users/katellguillou/Downloads/lyon_transport_stops_with_geometry.geojson\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 12:01:08,809 - INFO - Created 0 records\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to CSV: /Users/katellguillou/Downloads/lyon_transport_stops_with_geometry.csv\n",
      "\n",
      "Processed Data Summary:\n",
      "--------------------------------------------------\n",
      "Total features: 0\n",
      "Valid coordinates: 0\n",
      "\n",
      "Coordinate bounds:\n",
      "Latitude:  nan to nan\n",
      "Longitude: nan to nan\n",
      "\n",
      "Output files created:\n",
      "- CSV: /Users/katellguillou/Downloads/lyon_transport_stops_with_geometry.csv\n",
      "- GeoJSON: /Users/katellguillou/Downloads/lyon_transport_stops_with_geometry.geojson\n",
      "\n",
      "Sample of processed data:\n",
      "Empty GeoDataFrame\n",
      "Columns: [id, nom, desserte, pmr, ascenseur, escalator, gid, last_update, last_update_fme, adresse, localise_face_a_adresse, commune, insee, lon, lat, geometry]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import csv\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Define file paths\n",
    "input_file = Path.home() / 'Downloads' / 'points-arret-lignes-scolaires-reseau-transports-commun-lyonnais.csv'\n",
    "output_file = Path.home() / 'Downloads' / 'lyon_transport_stops_with_geometry.csv'\n",
    "\n",
    "def read_and_clean_csv(file_path):\n",
    "    \"\"\"Read CSV file with proper quoting and escaping\"\"\"\n",
    "    try:\n",
    "        # First, try to read with pandas using different settings\n",
    "        df = pd.read_csv(\n",
    "            file_path,\n",
    "            delimiter=';',\n",
    "            encoding='utf-8',\n",
    "            quoting=csv.QUOTE_ALL,  # Quote all fields\n",
    "            escapechar='\\\\',        # Use backslash as escape character\n",
    "            on_bad_lines='warn'     # Warn about problematic lines\n",
    "        )\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading CSV with pandas: {e}\")\n",
    "        \n",
    "        # Fallback: manually read and clean the CSV\n",
    "        try:\n",
    "            rows = []\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                # Read header\n",
    "                header = f.readline().strip().split(';')\n",
    "                \n",
    "                # Read and clean each line\n",
    "                for line in f:\n",
    "                    # Clean the line and split by delimiter\n",
    "                    cleaned_line = line.strip().replace('\"\"', '\"').split(';')\n",
    "                    if len(cleaned_line) == len(header):\n",
    "                        rows.append(cleaned_line)\n",
    "                    else:\n",
    "                        logging.warning(f\"Skipping malformed line: {line}\")\n",
    "            \n",
    "            return pd.DataFrame(rows, columns=header)\n",
    "        except Exception as e2:\n",
    "            logging.error(f\"Error in fallback CSV reading: {e2}\")\n",
    "            raise\n",
    "\n",
    "def process_data():\n",
    "    try:\n",
    "        # Read and clean the CSV file\n",
    "        print(\"Reading and cleaning CSV file...\")\n",
    "        df = read_and_clean_csv(input_file)\n",
    "        print(f\"Found {len(df)} rows\")\n",
    "        \n",
    "        # Convert coordinates to numeric values\n",
    "        df['lat'] = pd.to_numeric(df['lat'], errors='coerce')\n",
    "        df['lon'] = pd.to_numeric(df['lon'], errors='coerce')\n",
    "        \n",
    "        # Create geometry points\n",
    "        print(\"Creating geometry...\")\n",
    "        valid_coords = df['lat'].notna() & df['lon'].notna()\n",
    "        geometry = [Point(x, y) for x, y in zip(df.loc[valid_coords, 'lon'], df.loc[valid_coords, 'lat'])]\n",
    "        \n",
    "        # Create GeoDataFrame\n",
    "        gdf = gpd.GeoDataFrame(\n",
    "            df[valid_coords],\n",
    "            geometry=geometry,\n",
    "            crs=\"EPSG:4326\"\n",
    "        )\n",
    "        \n",
    "        # Save to GeoJSON (better for geometric features)\n",
    "        geojson_output = output_file.with_suffix('.geojson')\n",
    "        print(f\"Saving to GeoJSON: {geojson_output}\")\n",
    "        gdf.to_file(geojson_output, driver='GeoJSON')\n",
    "        \n",
    "        # Also save as CSV with WKT geometry\n",
    "        print(f\"Saving to CSV: {output_file}\")\n",
    "        gdf['geometry'] = gdf['geometry'].astype(str)\n",
    "        gdf.to_csv(output_file, index=False, encoding='utf-8', sep=';', quoting=csv.QUOTE_ALL)\n",
    "        \n",
    "        return gdf\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in data processing: {e}\")\n",
    "        raise\n",
    "\n",
    "def display_summary(gdf):\n",
    "    \"\"\"Display summary of the processed data\"\"\"\n",
    "    print(\"\\nProcessed Data Summary:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Total features: {len(gdf)}\")\n",
    "    print(f\"Valid coordinates: {gdf.geometry.notna().sum()}\")\n",
    "    print(\"\\nCoordinate bounds:\")\n",
    "    print(f\"Latitude:  {gdf['lat'].min():.6f} to {gdf['lat'].max():.6f}\")\n",
    "    print(f\"Longitude: {gdf['lon'].min():.6f} to {gdf['lon'].max():.6f}\")\n",
    "    \n",
    "    print(\"\\nOutput files created:\")\n",
    "    print(f\"- CSV: {output_file}\")\n",
    "    print(f\"- GeoJSON: {output_file.with_suffix('.geojson')}\")\n",
    "\n",
    "# Execute processing\n",
    "try:\n",
    "    print(\"Starting processing...\")\n",
    "    result_df = process_data()\n",
    "    display_summary(result_df)\n",
    "    \n",
    "    print(\"\\nSample of processed data:\")\n",
    "    print(result_df.head().to_string())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 12:02:11,475 - ERROR - Error in data processing: No valid features created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processing...\n",
      "Reading CSV file...\n",
      "Found 2143 rows\n",
      "Processing 0 valid coordinates...\n",
      "An error occurred: No valid features created\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No valid features created",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 104\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting processing...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 104\u001b[0m     gdf, geojson_data \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;66;03m# Validate and display results\u001b[39;00m\n\u001b[1;32m    107\u001b[0m     validate_geojson(geojson_data)\n",
      "Cell \u001b[0;32mIn[35], line 68\u001b[0m, in \u001b[0;36mprocess_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Validate GeoJSON structure\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(geojson_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo valid features created\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Save GeoJSON\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving to GeoJSON: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: No valid features created"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import json\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Define file paths\n",
    "input_file = Path.home() / 'Downloads' / 'points-arret-lignes-scolaires-reseau-transports-commun-lyonnais.csv'\n",
    "output_file = Path.home() / 'Downloads' / 'lyon_transport_stops_with_geometry.geojson'\n",
    "\n",
    "def create_geojson(df):\n",
    "    \"\"\"Create a properly structured GeoJSON from DataFrame\"\"\"\n",
    "    features = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        # Create feature properties from all columns except lat/lon\n",
    "        properties = row.drop(['lat', 'lon']).to_dict()\n",
    "        \n",
    "        # Create the feature\n",
    "        feature = {\n",
    "            \"type\": \"Feature\",\n",
    "            \"geometry\": {\n",
    "                \"type\": \"Point\",\n",
    "                \"coordinates\": [float(row['lon']), float(row['lat'])]\n",
    "            },\n",
    "            \"properties\": properties\n",
    "        }\n",
    "        features.append(feature)\n",
    "    \n",
    "    # Create the GeoJSON structure\n",
    "    geojson = {\n",
    "        \"type\": \"FeatureCollection\",\n",
    "        \"features\": features\n",
    "    }\n",
    "    \n",
    "    return geojson\n",
    "\n",
    "def process_data():\n",
    "    try:\n",
    "        # Read CSV file\n",
    "        print(\"Reading CSV file...\")\n",
    "        df = pd.read_csv(\n",
    "            input_file,\n",
    "            delimiter=';',\n",
    "            encoding='utf-8'\n",
    "        )\n",
    "        print(f\"Found {len(df)} rows\")\n",
    "        \n",
    "        # Convert coordinates to numeric\n",
    "        df['lat'] = pd.to_numeric(df['lat'], errors='coerce')\n",
    "        df['lon'] = pd.to_numeric(df['lon'], errors='coerce')\n",
    "        \n",
    "        # Remove rows with invalid coordinates\n",
    "        valid_coords = df['lat'].notna() & df['lon'].notna()\n",
    "        df = df[valid_coords]\n",
    "        \n",
    "        print(f\"Processing {len(df)} valid coordinates...\")\n",
    "        \n",
    "        # Create GeoJSON\n",
    "        geojson_data = create_geojson(df)\n",
    "        \n",
    "        # Validate GeoJSON structure\n",
    "        if len(geojson_data['features']) == 0:\n",
    "            raise ValueError(\"No valid features created\")\n",
    "        \n",
    "        # Save GeoJSON\n",
    "        print(f\"Saving to GeoJSON: {output_file}\")\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(geojson_data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        # Create GeoDataFrame for analysis\n",
    "        geometry = [Point(xy) for xy in zip(df['lon'], df['lat'])]\n",
    "        gdf = gpd.GeoDataFrame(df, geometry=geometry, crs=\"EPSG:4326\")\n",
    "        \n",
    "        return gdf, geojson_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in data processing: {e}\")\n",
    "        raise\n",
    "\n",
    "def validate_geojson(geojson_data):\n",
    "    \"\"\"Validate GeoJSON structure and content\"\"\"\n",
    "    print(\"\\nGeoJSON Validation:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Type: {geojson_data['type']}\")\n",
    "    print(f\"Number of features: {len(geojson_data['features'])}\")\n",
    "    \n",
    "    if len(geojson_data['features']) > 0:\n",
    "        sample_feature = geojson_data['features'][0]\n",
    "        print(\"\\nSample feature structure:\")\n",
    "        print(f\"- Feature type: {sample_feature['type']}\")\n",
    "        print(f\"- Geometry type: {sample_feature['geometry']['type']}\")\n",
    "        print(f\"- Number of properties: {len(sample_feature['properties'])}\")\n",
    "        print(\"\\nProperty names:\")\n",
    "        print(\", \".join(sample_feature['properties'].keys()))\n",
    "\n",
    "# Execute processing\n",
    "try:\n",
    "    print(\"Starting processing...\")\n",
    "    gdf, geojson_data = process_data()\n",
    "    \n",
    "    # Validate and display results\n",
    "    validate_geojson(geojson_data)\n",
    "    \n",
    "    print(\"\\nCoordinate Ranges:\")\n",
    "    print(f\"Latitude:  {gdf['lat'].min():.6f} to {gdf['lat'].max():.6f}\")\n",
    "    print(f\"Longitude: {gdf['lon'].min():.6f} to {gdf['lon'].max():.6f}\")\n",
    "    \n",
    "    print(f\"\\nOutput file created: {output_file}\")\n",
    "    print(f\"File size: {output_file.stat().st_size / 1024:.1f} KB\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processing...\n",
      "Reading CSV file...\n",
      "Found 2143 rows\n",
      "\n",
      "Coordinate Data Inspection:\n",
      "--------------------------------------------------\n",
      "\n",
      "First 5 raw coordinate values:\n",
      "Latitude values: ['45,80757185491305', '45,72432459823616', '45,856570219481455', '45,675036422408105', '45,70571954877312']\n",
      "Longitude values: ['4,809544825633801', '4,779963362674723', '4,876594070135879', '4,94924410028283', '4,865578537308746']\n",
      "\n",
      "Data types:\n",
      "Latitude: object\n",
      "Longitude: object\n",
      "\n",
      "Cleaning coordinates...\n",
      "Valid coordinates after cleaning: 2143\n",
      "Successfully created 2143 features\n",
      "Saving to GeoJSON: /Users/katellguillou/Downloads/lyon_transport_stops_with_geometry.geojson\n",
      "\n",
      "GeoJSON Validation:\n",
      "--------------------------------------------------\n",
      "Type: FeatureCollection\n",
      "Number of features: 2143\n",
      "\n",
      "Sample feature structure:\n",
      "- Feature type: Feature\n",
      "- Geometry type: Point\n",
      "- Sample coordinates: [4.809544825633801, 45.80757185491305]\n",
      "- Number of properties: 13\n",
      "\n",
      "Property names:\n",
      "id, nom, desserte, pmr, ascenseur, escalator, gid, last_update, last_update_fme, adresse, localise_face_a_adresse, commune, insee\n",
      "\n",
      "Coordinate Ranges:\n",
      "Latitude:  45.563672 to 45.937923\n",
      "Longitude: 4.456660 to 5.304569\n",
      "\n",
      "Output file created: /Users/katellguillou/Downloads/lyon_transport_stops_with_geometry.geojson\n",
      "File size: 1396.9 KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import json\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Define file paths\n",
    "input_file = Path.home() / 'Downloads' / 'points-arret-lignes-scolaires-reseau-transports-commun-lyonnais.csv'\n",
    "output_file = Path.home() / 'Downloads' / 'lyon_transport_stops_with_geometry.geojson'\n",
    "\n",
    "def inspect_coordinates(df):\n",
    "    \"\"\"Inspect coordinate data for debugging\"\"\"\n",
    "    print(\"\\nCoordinate Data Inspection:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"\\nFirst 5 raw coordinate values:\")\n",
    "    print(\"Latitude values:\", df['lat'].head().tolist())\n",
    "    print(\"Longitude values:\", df['lon'].head().tolist())\n",
    "    print(\"\\nData types:\")\n",
    "    print(\"Latitude:\", df['lat'].dtype)\n",
    "    print(\"Longitude:\", df['lon'].dtype)\n",
    "\n",
    "def clean_coordinates(value):\n",
    "    \"\"\"Clean coordinate string values\"\"\"\n",
    "    if pd.isna(value):\n",
    "        return None\n",
    "    try:\n",
    "        # Handle various string formats\n",
    "        if isinstance(value, str):\n",
    "            # Replace comma with dot for decimal\n",
    "            value = value.replace(',', '.')\n",
    "            # Remove any surrounding whitespace\n",
    "            value = value.strip()\n",
    "        return float(value)\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "\n",
    "def process_data():\n",
    "    try:\n",
    "        # Read CSV file\n",
    "        print(\"Reading CSV file...\")\n",
    "        df = pd.read_csv(\n",
    "            input_file,\n",
    "            delimiter=';',\n",
    "            encoding='utf-8'\n",
    "        )\n",
    "        print(f\"Found {len(df)} rows\")\n",
    "        \n",
    "        # Inspect raw data\n",
    "        inspect_coordinates(df)\n",
    "        \n",
    "        # Clean and convert coordinates\n",
    "        print(\"\\nCleaning coordinates...\")\n",
    "        df['lat'] = df['lat'].apply(clean_coordinates)\n",
    "        df['lon'] = df['lon'].apply(clean_coordinates)\n",
    "        \n",
    "        # Remove rows with invalid coordinates\n",
    "        valid_coords = df['lat'].notna() & df['lon'].notna()\n",
    "        df = df[valid_coords]\n",
    "        \n",
    "        print(f\"Valid coordinates after cleaning: {len(df)}\")\n",
    "        \n",
    "        if len(df) == 0:\n",
    "            raise ValueError(\"No valid coordinates found after cleaning\")\n",
    "        \n",
    "        # Create features list\n",
    "        features = []\n",
    "        for _, row in df.iterrows():\n",
    "            try:\n",
    "                # Create feature properties (excluding lat/lon)\n",
    "                properties = {\n",
    "                    col: row[col] \n",
    "                    for col in df.columns \n",
    "                    if col not in ['lat', 'lon']\n",
    "                }\n",
    "                \n",
    "                # Create feature\n",
    "                feature = {\n",
    "                    \"type\": \"Feature\",\n",
    "                    \"geometry\": {\n",
    "                        \"type\": \"Point\",\n",
    "                        \"coordinates\": [float(row['lon']), float(row['lat'])]\n",
    "                    },\n",
    "                    \"properties\": properties\n",
    "                }\n",
    "                features.append(feature)\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Error creating feature: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Create GeoJSON structure\n",
    "        geojson_data = {\n",
    "            \"type\": \"FeatureCollection\",\n",
    "            \"features\": features\n",
    "        }\n",
    "        \n",
    "        # Validate features\n",
    "        if len(features) == 0:\n",
    "            raise ValueError(\"No valid features created\")\n",
    "        \n",
    "        print(f\"Successfully created {len(features)} features\")\n",
    "        \n",
    "        # Save GeoJSON\n",
    "        print(f\"Saving to GeoJSON: {output_file}\")\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(geojson_data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        # Create GeoDataFrame for analysis\n",
    "        geometry = [Point(xy) for xy in zip(df['lon'], df['lat'])]\n",
    "        gdf = gpd.GeoDataFrame(df, geometry=geometry, crs=\"EPSG:4326\")\n",
    "        \n",
    "        return gdf, geojson_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in data processing: {e}\")\n",
    "        raise\n",
    "\n",
    "def validate_geojson(geojson_data):\n",
    "    \"\"\"Validate GeoJSON structure and content\"\"\"\n",
    "    print(\"\\nGeoJSON Validation:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Type: {geojson_data['type']}\")\n",
    "    print(f\"Number of features: {len(geojson_data['features'])}\")\n",
    "    \n",
    "    if len(geojson_data['features']) > 0:\n",
    "        sample_feature = geojson_data['features'][0]\n",
    "        print(\"\\nSample feature structure:\")\n",
    "        print(f\"- Feature type: {sample_feature['type']}\")\n",
    "        print(f\"- Geometry type: {sample_feature['geometry']['type']}\")\n",
    "        print(f\"- Sample coordinates: {sample_feature['geometry']['coordinates']}\")\n",
    "        print(f\"- Number of properties: {len(sample_feature['properties'])}\")\n",
    "        print(\"\\nProperty names:\")\n",
    "        print(\", \".join(sample_feature['properties'].keys()))\n",
    "\n",
    "# Execute processing\n",
    "try:\n",
    "    print(\"Starting processing...\")\n",
    "    gdf, geojson_data = process_data()\n",
    "    \n",
    "    # Validate and display results\n",
    "    validate_geojson(geojson_data)\n",
    "    \n",
    "    print(\"\\nCoordinate Ranges:\")\n",
    "    print(f\"Latitude:  {gdf['lat'].min():.6f} to {gdf['lat'].max():.6f}\")\n",
    "    print(f\"Longitude: {gdf['lon'].min():.6f} to {gdf['lon'].max():.6f}\")\n",
    "    \n",
    "    print(f\"\\nOutput file created: {output_file}\")\n",
    "    print(f\"File size: {output_file.stat().st_size / 1024:.1f} KB\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    raise\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
